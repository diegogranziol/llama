{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6ac4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Tuple, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairscale.nn.model_parallel.initialize import (\n",
    "    get_model_parallel_rank,\n",
    "    initialize_model_parallel,\n",
    "    model_parallel_is_initialized,\n",
    ")\n",
    "\n",
    "from llama.model import ModelArgs, Transformer\n",
    "from llama.tokenizer import Tokenizer\n",
    "\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d43c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CODE_OF_CONDUCT.md',\n",
       " 'USE_POLICY.md',\n",
       " 'example_text_completion.py',\n",
       " 'LICENSE',\n",
       " 'requirements.txt',\n",
       " 'MODEL_CARD.md',\n",
       " 'Responsible-Use-Guide.pdf',\n",
       " 'README.md',\n",
       " 'setup.py',\n",
       " '.gitignore',\n",
       " 'CONTRIBUTING.md',\n",
       " '.github',\n",
       " 'example_chat_completion.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'download.sh',\n",
       " 'Load Fresh Model.ipynb',\n",
       " 'venv',\n",
       " 'llama.egg-info',\n",
       " '.git',\n",
       " 'UPDATES.md',\n",
       " 'llama',\n",
       " '.idea']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed5009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "#     params = json.loads(f.read())\n",
    "\n",
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=128,\n",
    "    max_batch_size=1,\n",
    "#     **params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d149068",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "model parallel group is not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(model_args)\n",
      "File \u001b[0;32m~/Documents/llama/llama/model.py:450\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mn_layers\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_embeddings \u001b[38;5;241m=\u001b[39m ParallelEmbedding(\n\u001b[1;32m    451\u001b[0m     params\u001b[38;5;241m.\u001b[39mvocab_size, params\u001b[38;5;241m.\u001b[39mdim, init_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x\n\u001b[1;32m    452\u001b[0m )\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(params\u001b[38;5;241m.\u001b[39mn_layers):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fairscale/nn/model_parallel/layers.py:186\u001b[0m, in \u001b[0;36mParallelEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, init_method, keep_master_weight_for_test)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Divide the weight matrix along the embedding dimension.\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m world_size \u001b[38;5;241m=\u001b[39m get_model_parallel_world_size()\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim_per_partition \u001b[38;5;241m=\u001b[39m divide_and_check_no_remainder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim, world_size)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Allocate weights.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fairscale/nn/model_parallel/initialize.py:152\u001b[0m, in \u001b[0;36mget_model_parallel_world_size\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_parallel_world_size\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return world size for the model parallel group.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mget_world_size(group\u001b[38;5;241m=\u001b[39mget_model_parallel_group())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/fairscale/nn/model_parallel/initialize.py:128\u001b[0m, in \u001b[0;36mget_model_parallel_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_parallel_group\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mProcessGroup:\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the model parallel group the caller rank belongs to.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m _MODEL_PARALLEL_GROUP \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel parallel group is not initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _MODEL_PARALLEL_GROUP\n",
      "\u001b[0;31mAssertionError\u001b[0m: model parallel group is not initialized"
     ]
    }
   ],
   "source": [
    "model = Transformer(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd65a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
